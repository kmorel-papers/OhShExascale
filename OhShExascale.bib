%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for kmorel at 2013-01-09 17:08:17 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{Sewell2012,
	Author = {Christopher Sewell and Jeremy Meredith and Kenneth Moreland and Tom Peterka and Dave DeMarle and Li-ta Lo and James Ahrens and Robert Maynard and Berk Geveci},
	Booktitle = {Proceedings of the Ultrascale Visualization Workshop},
	Date-Added = {2013-01-09 17:08:13 -0700},
	Date-Modified = {2013-01-09 17:08:13 -0700},
	Month = {November},
	Title = {The {SDAV} Software Frameworks for Visualization and Analysis on Next-Generation Multi-Core and Many-Core Architectures},
	Year = {2012}}

@inproceedings{EAVL,
	Abstract = {The coming generation of supercomputing architectures will require fundamental changes in programming models to effectively make use of the expected million to billion way concurrency and thousand-fold reduction in per-core memory. Most current parallel analysis and visualization tools achieve scalability by partitioning the data, either spatially or temporally, and running serial computational kernels on each data partition, using message passing as needed. These techniques lack the necessary level of data parallelism to execute effectively on the underlying hardware. This paper introduces a framework that enables the expression of analysis and visualization algorithms with memory-efficient execution in a hybrid distributed and data parallel manner on both multi-core and many-core processors. We demonstrate results on scientific data using CPUs and GPUs in scalable heterogeneous systems.},
	Annote = {Provides an overview of EAVL, mult-core, many-core, GPU, exascale thinking system.  EAVL defines operations with the use of functors.  Current operations look limited to basic mapping operations that can look at topological connections.  The really unique thing about EAVL is its data model that can adapt to many topological features and can handle things like edge and face data.  It also seems like it can map pretty easily to these features.},
	Author = {Jeremy S. Meredith and Robert Sisneros and David Pugmire and Sean Ahern},
	Booktitle = {Proceedings of the 5th Annual Workshop on General Purpose Processing with Graphics Processing Units (GPGPU-5)},
	Date-Added = {2013-01-09 17:01:22 -0700},
	Date-Modified = {2013-01-09 17:01:22 -0700},
	Doi = {10.1145/2159430.2159432},
	Month = {March},
	Note = {{DOI}~10.1145/2159430.2159432},
	Pages = {11--19},
	Title = {A Distributed Data-Parallel Framework for Analysis and Visualization Algorithm Development},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2159430.2159432}}

@inproceedings{Moreland2011:LDAV,
	Annote = {Description of the Dax toolkit, a framework for building algorithms for GPU computers and, later, exascale computers.},
	Author = {Kenneth Moreland and Utkarsh Ayachit and Berk Geveci and Kwan-Liu Ma},
	Booktitle = {Proceedings of the IEEE Symposium on Large-Scale Data Analysis and Visualization},
	Date-Added = {2013-01-09 16:55:15 -0700},
	Date-Modified = {2013-01-09 16:55:15 -0700},
	Doi = {10.1109/LDAV.2011.6092323},
	Month = {October},
	Note = {{DOI}~10.1109/LDAV.2011.6092323},
	Pages = {97--104},
	Title = {Dax Toolkit: A Proposed Framework for Data Analysis and Visualization at Extreme Scale},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/LDAV.2011.6092323}}

@inproceedings{Moreland2012:PDAC,
	Author = {Kenneth Moreland and Brad King and Robert Maynard and Kwan-Liu Ma},
	Booktitle = {Petascale Data Analytics: Challenges and Opportunities (PDAC-12)},
	Date-Added = {2013-01-09 16:54:34 -0700},
	Date-Modified = {2013-01-09 16:54:34 -0700},
	Month = {November},
	Title = {Flexible Analysis Software for Emerging Architectures},
	Year = {2012}}

@techreport{PISTON,
	Abstract = {Due to the wide variety of current and next-generation supercomputing architectures, the development of highperformance parallel visualization and analysis operators frequently requires re-writing the underlying algorithms for many different platforms. In order to facilitate portability, we have devised a framework for creating such operators that employs the data-parallel programming model. By writing the operators using only data-parallel primitives (such as scans, transforms, stream compactions, etc.), the same code may be compiled to multiple targets using architecture-specific backend implementations of these primitives. Specifically, we make use of and extend NVIDIA's Thrust library, which provides CUDA and OpenMP backends. Using this framework, we have implemented isosurface, cut surface, and threshold operators, and have achieved good parallel performance on two different architectures (multi-core CPUs and NVIDIA GPUs) using the exact same operator code. We have applied these operators to several large, real scientific data sets, and have open-source released a beta version of our code base.},
	Annote = {A library providing portable implementations of visualization algorithms on multi- and many-core processors.  Uses Thrust to implement algorithms and portability.  Compares marching cubes isosurface generation to existing implementations by NVIDIA and VTK.

I believe this was presented at EGPGV 2012 and when it becomes available in the proceedings this reference should be updated.
},
	Author = {Li-Ta Lo and Chris Sewell and James Ahrens},
	Date-Added = {2013-01-09 16:47:35 -0700},
	Date-Modified = {2013-01-09 16:47:35 -0700},
	Institution = {Los Alamos National Laboratory},
	Number = {LA-UR-12-10227},
	Title = {{PISTON}: A Portable Cross-Platform Framework for Data-Parallel Visualization Operators},
	Url = {http://viz.lanl.gov/projects/PISTON.html},
	Year = {2012},
	Bdsk-Url-1 = {http://viz.lanl.gov/projects/PISTON.html}}

@book{Blelloch1990,
	Abstract = {Vector Models for Data-Parallel Computing describes a model of parallelism that extends and formalizes the Data-Parallel model on which the Connection Machine and other supercomputers are based. It presents many algorithms based on the model, ranging from graph algorithms to numerical algorithms, and argues that data-parallel models are not only practical and can be applied to a surprisingly wide variety of problems, they are also well suited for very-high-level languages and lead to a concise and clear description of algorithms and their complexity. Many of the author's ideas have been incorporated into the instruction set and into algorithms currently running on the Connection Machine.

The book includes the definition of a parallel vector machine; an extensive description of the uses of the scan (also called parallel-prefix) operations; the introduction of segmented vector operations; parallel data structures for trees, graphs, and grids; many parallel computational-geometry, graph, numerical and sorting algorithms; techniques for compiling nested parallelism; a compiler for Paralation Lisp; and details on the implementation of the scan operations.},
	Annote = {Seminal work on using scans and other basic parallel algorithms to build more specific data-parallel algorithms.},
	Author = {Guy E. Blelloch},
	Date-Added = {2013-01-09 16:42:04 -0700},
	Date-Modified = {2013-01-09 16:42:04 -0700},
	Note = {{ISBN}~0-262-02313-X},
	Publisher = {MIT Press},
	Title = {Vector Models for Data-Parallel Computing},
	Year = {1990}}

@inproceedings{Baker2010,
	Abstract = {Multicore nodes have become ubiquitous in just a few years. At the same time, writing portable parallel software for multicore nodes is extremely challenging. Widely available programming models such as OpenMP and Pthreads are not useful for devices such as graphics cards, and more flexible programming models such as RapidMind are only available commercially. OpenCL represents the first truly portable standard, but its availability is limited. In the presence of such transition, we have developed a minimal application programming interface (API) for multicore nodes that allows us to write portable parallel linear algebra software that can use any of the aforementioned programming models and any future standard models. We utilize C++ template meta-programming to enable users to write parallel kernels that can be executed on a variety of node types, including Cell, GPUs and multicore CPUs. The support for a parallel node is provided by implementing a Node object, according to the requirements specified by the API. This ability to provide custom support for particular node types gives developers a level of control not allowed by the current slate of proprietary parallel programming APIs. We demonstrate implementations of the API for a simple vector dot-product on sequential CPU, multicore CPU and GPU nodes.},
	Annote = {This is a paper from Mike Heroux and gang about the multi-core parallel technique using functors.  It makes for a vary portable system parallel system and this is demonstrated on intel threaded building blocks (TBB) and CUDA.},
	Author = {Christopher G. Baker and Michael A. Heroux and H. Carter Edwards and Alan B. Williams},
	Booktitle = {Proceedings of the 18th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)},
	Date-Added = {2013-01-09 16:40:50 -0700},
	Date-Modified = {2013-01-09 16:40:50 -0700},
	Doi = {10.1109/PDP.2010.49},
	Month = {February},
	Note = {{DOI}~10.1109/PDP.2010.49},
	Pages = {601 -- 606},
	Title = {A Light-weight {API} for Portable Multicore Programming},
	Url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5452412},
	Year = {2010},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5452412},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/PDP.2010.49}}

@incollection{Li2008,
	Annote = {A general paper on optimizing parallel algorithms for use in hybrid MPI+OpenMP/threaded systems.},
	Author = {Li Chen and Issei Fujishiro},
	Booktitle = {A Practical Programming Model for the Multi-Core Era},
	Date-Added = {2013-01-09 16:26:26 -0700},
	Date-Modified = {2013-01-09 16:26:26 -0700},
	Doi = {10.1007/978-3-540-69303-1\_10},
	Note = {{DOI}~10.1007/978-3-540-69303-1\_10},
	Pages = {112--124},
	Publisher = {Springer},
	Title = {Optimization Strategies Using Hybrid {MPI}+{OpenMP} Parallelization for Large-Scale Data Visualization on Earth Simulator},
	Volume = {4935},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-540-69303-1%5C_10}}

@book{Sanders2011,
	Annote = {A book describing how to use CUDA including a lot of practical advice.},
	Author = {Jason Sanders and Edward Kandrot},
	Date-Added = {2013-01-09 16:19:52 -0700},
	Date-Modified = {2013-01-09 16:19:52 -0700},
	Note = {{ISBN}~978-0-13-138768-3},
	Publisher = {Addison Wesley},
	Title = {{CUDA} by Example},
	Year = {2011}}

@book{TBB,
	Annote = {The user's guide and reference book for Intel Threading Building Blocks (TBB).  Also contains lots of practical advice on creating efficient application including notes on locking, allocations, cache-lines, and false-sharing.},
	Author = {James Reinders},
	Date-Added = {2013-01-09 16:16:39 -0700},
	Date-Modified = {2013-01-09 16:16:39 -0700},
	Month = {July},
	Note = {{ISBN}~978-0-596-51480-8},
	Publisher = {O'Reilly},
	Title = {Intel Threading Building Blocks: Outfitting {C++} for Multi-core Processor Parallelism},
	Year = {2007}}

@techreport{ParaViewTutorial,
	Annote = {The ParaView tutorial.},
	Author = {Kenneth Moreland},
	Date-Added = {2013-01-09 15:15:38 -0700},
	Date-Modified = {2013-01-09 15:18:10 -0700},
	Institution = {Sandia National Laboratories},
	Number = {SAND 2011-8896P},
	Title = {The {ParaView} Tutorial, Version 3.12},
	Year = {2011}}

@article{Moreland2012:TVCG,
	Abstract = {The most common abstraction used by visualization libraries and applications today is what is known as the visualization pipeline. The visualization pipeline provides a mechanism to encapsulate algorithms and then couple them together in a variety of ways. The visualization pipeline has been in existence for over twenty years, and over this time many variations and improvements have been proposed. This paper provides a literature review of the most prevalent features of visualization pipelines and some of the most recent research directions.},
	Annote = {A survey of research pertaining to visualization pipelines, the libraries that implement them, and the tools that use them.},
	Author = {Kenneth Moreland},
	Date-Added = {2013-01-09 15:12:24 -0700},
	Date-Modified = {2013-01-09 15:12:24 -0700},
	Journal = {IEEE Transactions on Visualization and Computer Graphics},
	Note = {(preprint)},
	Title = {A Survey of Visualization Pipelines},
	Year = {2012}}

@techreport{DARPAExascaleStudy,
	Annote = {An exascale workshop study.  I've specifically used this one to pull out the following quote to demonstrate the limitation of memory that we have at the exascale:

  The concurrency challenge is manifest in the need for software to expose
  at least 1000$\times$ more concurrency in applications for Extreme Scale
  systems, relative to current systems. It is further exacerbated by the
  projected memory-computation imbalances in Extreme Scale systems, with
  Bytes/Ops ratios that may drop to values as low as $10^{-2}$ where Bytes
  and Ops represent the main memory and computation capacities of the
  system respectively. These ratios will result in 100$\times$ reductions
  in memory per core relative to Petascale systems, with accompanying
  reductions in memory bandwidth per core.  Thus, a significant fraction of
  software concurrency in Extreme Scale systems must come from exploiting
  more parallelism within the computation performed on a single datum.
},
	Author = {Mark Richards and others},
	Date-Added = {2013-01-09 15:11:05 -0700},
	Date-Modified = {2013-01-09 15:11:05 -0700},
	Institution = {DARPA Information Processing Techniques Office (IPTO)},
	Month = {September},
	Title = {ExaScale Software Study: Software Challenges in Extreme Scale Systems},
	Year = {2009}}

@article{Gustafson1988,
	Annote = {A paper by Gustafson describing how efficient parallel processing can be achieved by scalling up the size of the problem with the amount of parallelism.  This is in contrast to Amdahl's law, which limits the amount of speedup that can be achieved.  Provides the equation that is now known as Gustafson's law (or Gustafson-Barsis's law).},
	Author = {John L. Gustafson},
	Date-Added = {2013-01-09 14:41:18 -0700},
	Date-Modified = {2013-01-09 15:20:57 -0700},
	Doi = {10.1145/42411.42415},
	Journal = {Communications of the ACM},
	Month = {May},
	Note = {{DOI}~10.1145/42411.42415},
	Number = {5},
	Pages = {532--533},
	Title = {Reevaluating {Amdahl's} Law},
	Volume = {31},
	Year = {1988},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/42411.42415}}

@inproceedings{Amdahl1967,
	Annote = {Amdahl's original (?) short paper describing the limitations of amount of increased speedup you get with parallel computing.  I note that the equation now associated with Amdahl's law is not given here.},
	Author = {Gene M. Amdahl},
	Booktitle = {Proceedings of the Spring Joint Computer Conference (AFIPS '67)},
	Date-Added = {2013-01-09 14:33:18 -0700},
	Date-Modified = {2013-01-09 14:33:18 -0700},
	Doi = {10.1145/1465482.1465560},
	Month = {April},
	Note = {{DOI}~10.1145/1465482.1465560},
	Pages = {483--485},
	Title = {Validity of the single processor approach to achieving large scale computing capabilities},
	Year = {1967},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/1465482.1465560}}

@book{Quinn2004,
	Annote = {A book containing mostly practical algorithms and their implementations using MPI and OpenMP.  The book also contains what I think is a nice review of some of the basic theory of parallel computing including Amdahl's law, Gustafson-Barsis's law, the Karp-Flatt metric, and the Isoefficiency metric.},
	Author = {Michael J. Quinn},
	Date-Added = {2013-01-09 14:25:42 -0700},
	Date-Modified = {2013-01-09 14:25:42 -0700},
	Note = {{ISBN}~978-0-07-282256-4},
	Publisher = {McGraw-Hill},
	Title = {Parallel Programming in C with MPI and OpenMP},
	Year = {2004}}

@book{MPI,
	Annote = {My reference manual for MPI.  I've been using this as a cite for MPI.},
	Author = {Mark Snir and Steve Otto and Steven Huss-Lederman and David Walker and Jack Dongarra},
	Date-Added = {2013-01-09 13:55:07 -0700},
	Date-Modified = {2013-01-09 13:55:07 -0700},
	Edition = {second},
	Note = {{ISBN} 0-262-69215-5},
	Publisher = {MIT Press},
	Title = {{MPI}: The Complete Reference},
	Volume = {1, The {MPI} Core},
	Year = {1998}}

@misc{ScientificDiscoveryExascale2011,
	Annote = {Workshop report on upcoming visualization challenges at exascale.  Big challenges include overcoming lower relative I/O rates through various more advanced I/O technologies such as in situ computations.  Also includes discussion on upcoming algorithmic challenges.},
	Author = {Sean Ahern and Arie Shoshani and Kwan-Liu Ma and others},
	Date-Added = {2013-01-09 11:29:04 -0700},
	Date-Modified = {2013-01-09 11:29:04 -0700},
	Howpublished = {Report from the DOE ASCR 2011 Workshop on Exascale Data Management, Analysis, and Visualization},
	Month = {February},
	Title = {Scientific Discovery at the Exascale},
	Url = {http://www.sandia.gov/~kmorel/documents/Exascale-ASCR-Analysis.pdf},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sandia.gov/~kmorel/documents/Exascale-ASCR-Analysis.pdf}}

@techreport{ExascaleRoadMap,
	Annote = {A workshop report prediciting the nature of an Exascale machine.  A reasonable source to pull some numbers comparing petascale to exascale machines (a la slide of doom).  See [Moreland2011:LDAV] for a usage.},
	Author = {Jack Dongarra and Pete Beechman and others},
	Date-Added = {2013-01-09 11:25:57 -0700},
	Date-Modified = {2013-01-09 11:25:57 -0700},
	Institution = {University of Tennessee},
	Month = {January},
	Number = {ut-cs-10-652},
	Title = {The International Exascale Software Project RoadMap},
	Url = {http://www.cs.utk.edu/~library/TechReports/2010/ut-cs-10-652.pdf},
	Year = {2010},
	Bdsk-Url-1 = {http://www.cs.utk.edu/~library/TechReports/2010/ut-cs-10-652.pdf}}

@techreport{ExascaleArchitecturesReport,
	Adsurl = {http://science.energy.gov/~/media/ascr/pdf/program-documents/docs/Arch_tech_grand_challenges_report.pdf},
	Annote = {Another workshop report for predicting exascale computers and the necessary changes for it.},
	Author = {Rick Stevens and Andrew White and others},
	Date-Added = {2013-01-09 11:25:40 -0700},
	Date-Modified = {2013-01-09 11:25:40 -0700},
	Institution = {ASCR Scientific Grand Challenges Workshop Series},
	Month = {December},
	Title = {Architectures and Technology for Extreme Scale Computing},
	Year = {2009}}

@article{Wylie2001,
	Annote = {Early article demonstrating the good scaling performance of sort-last parallel rendering.  (Does not really compare to other parallel rendering methods, although those are known to scale poorly.  You can use [Mueller1995] to argue poor scaling of sort-first.)},
	Author = {Brian Wylie and Constantine Pavlakos and Vasily Lewis and Kenneth Moreland},
	Date-Added = {2013-01-09 11:19:22 -0700},
	Date-Modified = {2013-01-09 11:19:22 -0700},
	Journal = {IEEE Computer Graphics and Applications},
	Month = {July/August},
	Number = {4},
	Pages = {62--70},
	Title = {Scalable Rendering on {PC} Clusters},
	Volume = {21},
	Year = {2001}}

@techreport{Ahrens2000,
	Annote = {One of the seminal papers on introducing parallelism and data streaming into VTK specifically and into pipelines in general.},
	Author = {James Ahrens and Charles Law and Will Schroeder and Ken Martin and Michael Papka},
	Date-Added = {2013-01-09 11:18:51 -0700},
	Date-Modified = {2013-01-09 11:18:51 -0700},
	Institution = {Los Alamos National Laboratory},
	Number = {\#LAUR-00-1620},
	Title = {A Parallel Approach for Efficiently Visualizing Extremely Large, Time-Varying Datasets},
	Year = {2000}}

@article{Childs2010,
	Annote = {Explores the scaling of visualization pipelines (VisIt specifically) on full petascale machines.},
	Author = {Hank Childs and David Pugmire and Sean Ahern and Brad Whitlock and Mark Howison and Prabhat and Gunther H. Weber and E. Wes Bethel},
	Date-Added = {2013-01-09 11:14:54 -0700},
	Date-Modified = {2013-01-09 11:14:54 -0700},
	Doi = {10.1109/MCG.2010.51},
	Journal = {IEEE Computer Graphics and Applications},
	Month = {May/June},
	Note = {{DOI}~10.1109/MCG.2010.51},
	Number = {3},
	Pages = {22--31},
	Title = {Extreme Scaling of Production Visualization Software on Diverse Architectures},
	Volume = {30},
	Year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/MCG.2010.51}}

@manual{VisIt,
	Annote = {Dude, it's VisIt.},
	Date-Added = {2013-01-09 11:09:30 -0700},
	Date-Modified = {2013-01-09 11:09:30 -0700},
	Month = {October},
	Note = {Technical Report UCRL-SM-220449},
	Organization = {Lawrence Livermore National Laboratory},
	Title = {VisIt User's Manual},
	Year = {2005}}

@book{ParaView,
	Annote = {Dude, its ParaView.},
	Author = {Utkarsh Ayachit and others},
	Date-Added = {2013-01-09 11:09:20 -0700},
	Date-Modified = {2013-01-09 11:09:20 -0700},
	Edition = {4th},
	Note = {{ISBN} 978-1-930934-24-5},
	Publisher = {Kitware Inc.},
	Title = {The {ParaView} Guide: A Parallel Visualization Application},
	Url = {http://www.paraview.org},
	Year = {2012},
	Bdsk-Url-1 = {http://www.paraview.org}}
